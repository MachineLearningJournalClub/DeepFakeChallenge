{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPU_Pipeline_MLJC_(1) (1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "IjbJ9FP3KGZF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import print_function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iecYGc8GShLJ",
        "colab_type": "text"
      },
      "source": [
        "## TPU Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BN2Z2NPzNltq",
        "colab_type": "code",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "outputId": "c1eb4d67-aaf9-4be5-e9ef-364af2c2638e"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "Tensorflow version 2.1.0\n",
            "Running on TPU  ['10.99.172.2:8470']\n",
            "INFO:tensorflow:Initializing the TPU system: 10.99.172.2:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: 10.99.172.2:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLQq46_WSdDx",
        "colab_type": "text"
      },
      "source": [
        "## Some functions we might need (Siamese)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsp7piI2ODjs",
        "colab_type": "code",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d0c085dc-eecb-43c1-94f8-720e96c56afb"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "\n",
        "import random\n",
        "from keras.datasets import mnist\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, Lambda\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "\n",
        "def euclidean_distance(vects):#vects\n",
        "    x, y = vects\n",
        "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
        "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
        "\n",
        "\n",
        "def eucl_dist_output_shape(shapes):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1[0], 1)\n",
        "\n",
        "\n",
        "def contrastive_loss(y_true, y_pred):\n",
        "    '''Contrastive loss from Hadsell-et-al.'06\n",
        "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
        "    '''\n",
        "    margin = 1 #1.25\n",
        "    square_pred = K.square(y_pred)\n",
        "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
        "    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    '''Compute classification accuracy with a fixed threshold on distances.\n",
        "    '''\n",
        "    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BonCXFESkPj",
        "colab_type": "text"
      },
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAVaYPTYRvgY",
        "colab_type": "code",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "353e265f-c970-464d-cdd9-17b654cbaff8"
      },
      "source": [
        "!pip install efficientnet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: efficientnet in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /tensorflow-2.1.0/python3.6 (from efficientnet) (1.0.8)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from efficientnet) (0.16.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /tensorflow-2.1.0/python3.6 (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.18.1)\n",
            "Requirement already satisfied: h5py in /tensorflow-2.1.0/python3.6 (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (2.10.0)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet) (1.1.1)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /tensorflow-2.1.0/python3.6 (from scikit-image->efficientnet) (1.4.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet) (3.1.3)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet) (2.4)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet) (6.2.2)\n",
            "Requirement already satisfied: six in /tensorflow-2.1.0/python3.6 (from h5py->keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (2.6.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (2.4.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (1.1.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image->efficientnet) (4.4.1)\n",
            "Requirement already satisfied: setuptools in /tensorflow-2.1.0/python3.6 (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (45.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHuoIjgFSkYw",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "#from tensorflow.keras.applications.xception import Xception\n",
        "import efficientnet.tfkeras as efn \n",
        "\n",
        "\n",
        "\"\"\"\n",
        "def create_model():\n",
        "  input_shape=(313,214,3)\n",
        "\n",
        "  xception = Xception(weights='imagenet', include_top=False,input_shape=input_shape,pooling='avg')\n",
        "\n",
        "  # network definition\n",
        "  base_network = xception\n",
        "\n",
        "  input_a = Input(shape=input_shape)\n",
        "  input_b = Input(shape=input_shape)\n",
        "\n",
        "  # because we re-use the same instance `base_network`,\n",
        "  # the weights of the network\n",
        "  # will be shared across the two branches\n",
        "  processed_a = base_network(input_a)\n",
        "  processed_b = base_network(input_b)\n",
        "\n",
        "  distance = Lambda(euclidean_distance,\n",
        "                    output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
        "\n",
        "  return Model([input_a, input_b], distance)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def create_model():\n",
        "    input_shape=(317,232,3)\n",
        "\n",
        "    effNet = efn.EfficientNetB4(weights= None, include_top=False, input_shape=input_shape, pooling=\"avg\")  # or weights='imagenet' , 'noisy-student'\n",
        "\n",
        "    #xception = Xception(weights='imagenet', include_top=False,input_shape=input_shape,pooling='avg')\n",
        "    \n",
        "        \n",
        "    \n",
        "    # network definition\n",
        "    base_network = effNet #xception\n",
        "\n",
        "    \n",
        "\n",
        "    input_a = Input(shape=input_shape)\n",
        "    input_b = Input(shape=input_shape)\n",
        "\n",
        "    # because we re-use the same instance `base_network`,\n",
        "    # the weights of the network\n",
        "    # will be shared across the two branches\n",
        "    processed_a = base_network(input_a)\n",
        "    processed_b = base_network(input_b)\n",
        "\n",
        "    distance = Lambda(euclidean_distance,\n",
        "                    output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
        "\n",
        "    return Model([input_a, input_b], distance)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bmDzi2i58rZ",
        "colab_type": "text"
      },
      "source": [
        "### Network Surgery \n",
        "Head Layer for classification  with EfficientNet (**post Siamese training**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRZyg7jK57Ub",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "class FCHeadNet:\n",
        "    @staticmethod\n",
        "    def build(baseModel, classes, D):\n",
        "        #Initialize the hed model that will be placed on top of the base, then add a FC layer\n",
        "        headModel = baseModel.output\n",
        "        headModel = Flatten(name = \"flatten\")(headModel)\n",
        "        headModel = Dense(D, activation = \"relu\")(headModel)\n",
        "        headModel = Dropout(0.5)(headModel)\n",
        "\n",
        "        # Add a softmax layer (in our case logistic should be the same)\n",
        "        headModel = Dense(classes, activation = \"softmax\")(headModel)\n",
        "\n",
        "        return headModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSgADU4LSYS5",
        "colab_type": "text"
      },
      "source": [
        "## Accessing GS Bucket"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVsOdQjHOhRH",
        "colab_type": "code",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d8ca2e7c-4afb-4ca3-9bd8-0a31d9831fa0"
      },
      "source": [
        "### Configuring our gs bucket (cloud storage)\n",
        "\n",
        "# Set your own project id here\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "PROJECT_ID = 'deepfakechallenge-269819'\n",
        "from google.cloud import storage\n",
        "storage_client = storage.Client(project=PROJECT_ID)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ifEExSEOnEB",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "bucket_name = \"deepfake_mljc_bucket\"\n",
        "gcs_pattern = 'gs://deepfake_mljc_bucket/TFRecs/Training/*.tfrecords'\n",
        "gcs_valid_pattern = 'gs://deepfake_mljc_bucket/TFRecs/Validation/df0.tfrecords'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSq6akfbSsgQ",
        "colab_type": "text"
      },
      "source": [
        "### Checkpointing our Model\n",
        "\n",
        "Every 1000 iterations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NeO5ZdoSrWw",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "# It saves the model every 1000 epochs --> to be lately put in the tpu_model as soon as it works \n",
        "import os\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint_path = \"gs://deepfake_mljc_bucket/Checkpoints/cp_tpu.ckpt\"\n",
        "#checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create checkpoint callback\n",
        "cp_callback = ModelCheckpoint(checkpoint_path, save_freq= 1000,\n",
        "                                                 verbose=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k32GPBK7Y-yo",
        "colab_type": "code",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "ffa940c8-4b78-4c0a-a3d9-5b8a640f8b50"
      },
      "source": [
        "!gsutil ls gs://deepfake_mljc_bucket/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gs://deepfake_mljc_bucket/Audio/\n",
            "gs://deepfake_mljc_bucket/Checkpoints/\n",
            "gs://deepfake_mljc_bucket/FacesDataframes/\n",
            "gs://deepfake_mljc_bucket/TFRecs/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMW7RkS96lHV",
        "colab_type": "text"
      },
      "source": [
        "## TFRecords "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjAiw8Wc6j5C",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "#Training Set\n",
        "filenames = tf.io.gfile.glob(gcs_pattern)\n",
        "#Validation Set\n",
        "validation_fns = tf.io.gfile.glob(gcs_valid_pattern)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "WhwNuSdYKGaD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9aeb8fad-7485-438c-c888-5a0788a20c55"
      },
      "source": [
        "filenames[35]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'gs://deepfake_mljc_bucket/TFRecs/Training/df41.tfrecords'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEdNfLtjKi5S",
        "colab_type": "text"
      },
      "source": [
        "### Training Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRprbryeytjU",
        "colab_type": "code",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "cf82d95c-3d6d-435f-caad-705bfb0647d7"
      },
      "source": [
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "ignore_order = tf.data.Options()\n",
        "ignore_order.experimental_deterministic = False\n",
        "\n",
        "#filenames = tf.io.gfile.glob(FILENAME_PATTERN)\n",
        "dataset = tf.data.TFRecordDataset(filenames[35], num_parallel_reads=AUTO)\n",
        "#dataset = dataset.with_options(ignore_order)\n",
        "\n",
        "BATCH_SIZE = 128 #16 * tpu_strategy.num_replicas_in_sync #256\n",
        "PREFETCH_SIZE= 256 #32 * tpu_strategy.num_replicas_in_sync #512 \n",
        "\n",
        "def read_tfrecord(data):\n",
        "    image_feature_description = {\n",
        "\n",
        "      'filename_1': tf.io.FixedLenFeature([], tf.string),\n",
        "      'filename_2': tf.io.FixedLenFeature([], tf.string),\n",
        "\n",
        "      #'width': tf.io.FixedLenFeature([], tf.int64),\n",
        "      #'height': tf.io.FixedLenFeature([], tf.int64),\n",
        "\n",
        "      #'channels': tf.io.FixedLenFeature([], tf.int64),\n",
        "      'image_1': tf.io.FixedLenFeature([], tf.string),\n",
        "      'image_2': tf.io.FixedLenFeature([], tf.string),\n",
        "      'label': tf.io.FixedLenFeature([], tf.float32),\n",
        "      }\n",
        "\n",
        "    tf_record = tf.io.parse_single_example(data, image_feature_description)\n",
        "    img_height = 317\n",
        "    img_width = 232\n",
        "    img_size = (img_height, img_width)\n",
        "\n",
        "    # ISSUE WITH TF.IMAGE.RESIZE\n",
        "    # https://hackernoon.com/how-tensorflows-tf-image-resize-stole-60-days-of-my-life-aba5eb093f35 \n",
        "\n",
        "    image_1 = tf.image.decode_png(tf_record[\"image_1\"], channels = 3)\n",
        "    # Keep it commented if you're using padded and cropped data since the dimension is the default one\n",
        "    image_1 = tf.image.resize(image_1, img_size)\n",
        "    image_1 = tf.cast(image_1, tf.float32)\n",
        "    #Teoricamente non dovrebbe servire nemmeno la standardization, siccome nella creazione dei tf records si è usato tf.image.decode_png --> i valori dei pixel sono float in [0,1]\n",
        "    image_1 = tf.image.per_image_standardization(image_1)\n",
        "\n",
        "\n",
        "    image_2 = tf.image.decode_png(tf_record[\"image_2\"], channels = 3,)\n",
        "\n",
        "    # Keep it commented if you're using padded and cropped data since the dimension is the default one\n",
        "    image_2 = tf.image.resize(image_2, img_size)\n",
        "    image_2 = tf.cast(image_2, tf.float32)\n",
        "    image_2 = tf.image.per_image_standardization(image_2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    label = tf_record[\"label\"]\n",
        "    #label = tf.cast(label, tf.int32)  \n",
        "\n",
        "    #def _parse_image_function(example_proto):\n",
        "    # Parse the input tf.Example proto using the dictionary above.\n",
        "    # return tf.io.parse_single_example(example_proto, image_feature_description)\n",
        "\n",
        "\n",
        "    return (image_1, image_2), label\n",
        "\n",
        "dataset = dataset.map(read_tfrecord)\n",
        "dataset = dataset.shuffle(BATCH_SIZE*16, reshuffle_each_iteration=True) \n",
        "\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True).prefetch(PREFETCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/image_ops_impl.py:1556: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/image_ops_impl.py:1556: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUFJdlpA10Uo",
        "colab_type": "code",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1b12694a-6230-4151-dd57-98d145cd90b5"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: (((128, 317, 232, 3), (128, 317, 232, 3)), (128,)), types: ((tf.float32, tf.float32), tf.float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjFjATC-K2Ve",
        "colab_type": "text"
      },
      "source": [
        "### Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X32tbcgnK5ZV",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "valid_set = tf.data.TFRecordDataset(validation_fns, num_parallel_reads=AUTO)\n",
        "valid_set= valid_set.map(read_tfrecord)\n",
        "valid_set = valid_set.shuffle(BATCH_SIZE*16, reshuffle_each_iteration=True) \n",
        "\n",
        "valid_set = valid_set.batch(BATCH_SIZE, drop_remainder=True).prefetch(PREFETCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhJdNVG8LrEi",
        "colab_type": "code",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "56fb52fc-b296-4907-c3bc-9a5735f99986"
      },
      "source": [
        "valid_set"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: (((128, 317, 232, 3), (128, 317, 232, 3)), (128,)), types: ((tf.float32, tf.float32), tf.float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCq6cFwZxsd5",
        "colab_type": "text"
      },
      "source": [
        "## TPU Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcPd2RV8x5Co",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "#learning_rate = 0.1\n",
        "\n",
        "#lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate = learning_rate, decay_steps= (800), decay_rate= 1e-4)\n",
        "#opt = tf.keras.optimizers.SGD(learning_rate= lr_schedule, momentum = 0.9)\n",
        "opt = tf.keras.optimizers.SGD(learning_rate= 0.01) #, momentum = 0.9, nesterov= True)\n",
        "\n",
        "#checkpoint_path = \"gs://deepfake_mljc_bucket/Checkpoints/cp_tpu1.ckpt\"\n",
        "checkpoint_path = \"gs://deepfake_mljc_bucket/Checkpoints/Margin1/cp_tpu1.ckpt\"\n",
        "with tpu_strategy.scope():\n",
        "    model = create_model()\n",
        "    \n",
        "    for layer in  model.layers[2].layers[:-326]:\n",
        "      layer.trainable = False \n",
        "    #for layer in model.layers[2].layers[:-237]:\n",
        "      #layer.trainable = False \n",
        "\n",
        "    model.compile(\n",
        "    optimizer = opt,\n",
        "    loss=contrastive_loss,\n",
        "    metrics=[accuracy]) \n",
        "\n",
        "    model.load_weights(checkpoint_path)\n",
        "    \n",
        "# class_weight = {'0.0': 0.27586, '1.0': 0.72414}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1saJ0ZPzga0",
        "colab_type": "code",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "a00005c2-43ce-43b7-8a82-d6e21b11883d"
      },
      "source": [
        "history = model.fit(dataset, epochs= 5, shuffle = True)#, validation_data= valid_set, validation_freq=5) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "666/666 [==============================] - 1353s 2s/step - loss: 0.1522 - accuracy: 0.7751\n",
            "Epoch 2/5\n",
            "  1/666 [..............................] - ETA: 19:41"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "UnavailableError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnavailableError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-32deefde4e0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, validation_data= valid_set, validation_freq=5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36m_non_none_constant_value\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_non_none_constant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m   \u001b[0mconstant_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mconstant_value\u001b[0;34m(tensor, partial)\u001b[0m\n\u001b[1;32m    820\u001b[0m   \"\"\"\n\u001b[1;32m    821\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    940\u001b[0m     \"\"\"\n\u001b[1;32m    941\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    908\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mUnavailableError\u001b[0m: Socket closed"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZn418BVaeBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = \"gs://deepfake_mljc_bucket/Checkpoints/Margin1/cp_tpu1.ckpt\"\n",
        "model.save_weights(checkpoint_path, overwrite= True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRL5_MQ3CkGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}